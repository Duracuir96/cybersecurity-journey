## ğŸ“Š Analytics Pipelines (Sales & Stock)

The Analytics layer of **SalesFlow Lite** is implemented as **two distinct but structurally consistent backend pipelines**:

* **Sales Analytics Pipeline**
* **Stock Analytics Pipeline**

Both pipelines follow an **ETL-like architecture**:

* **Extract** data from the Java backend
* **Transform** data using Pandas and domain logic
* **Load / Serve** results via Redis cache and FastAPI APIs

They are fully automated, stateless at runtime, and secured through JWT-based service-to-service communication.

---

## ğŸ§© Common Pipeline Characteristics

* Triggered **on-demand via API**
* Fully automated (no manual steps)
* Uses **Java â†’ Python microservice orchestration**
* Strongly typed I/O via **Pydantic DTOs**
* Cached results for performance and resilience
* Deterministic and reproducible outputs

---

## ğŸ”¹ Sales Analytics Pipeline

### ğŸ¯ Purpose

Provide aggregated sales insights for business decision-making:

* Revenue evolution
* Transaction volume
* Product performance
* Daily sales trends

---

### ğŸ” Pipeline Flow

```
API Request
   â†“
JWT extraction & validation
   â†“
Java Sales + Products APIs
   â†“
Data normalization (dict conversion)
   â†“
Date filtering (period / custom range)
   â†“
Aggregation with Pandas-like logic
   â†“
KPI computation
   â†“
Redis cache
   â†“
FastAPI response (typed DTO)
```

---

### ğŸ§  Implementation â€“ Core Logic

ğŸ“ **File:** `src/services/analytics_service.py`

#### Data extraction & orchestration

```python
prod = JavaProductsClient(token)
sales = JavaSalesClient(token)

products = [to_dict(p) for p in await prod.get_all_products()]
sales_history = [to_dict(s) for s in await sales.get_sales_history()]
```

âœ” Automated service-to-service calls
âœ” JWT forwarded from the API layer
âœ” Clean resource handling with `close()`

---

#### Transformation & aggregation

```python
daily = defaultdict(lambda: {"rev": 0.0, "qty": 0.0, "tx": 0})
agg = defaultdict(lambda: {"rev": 0.0, "qty": 0.0})

for sale in sales_history:
    d = parse_date(sale.get("saleDate"))
    if not d or d < start_date or d > end_date:
        continue

    total_amount = float(sale.get("totalAmount", 0))
    daily[d]["rev"] += total_amount
    daily[d]["tx"] += 1
```

âœ” Time-window filtering
âœ” Deterministic aggregation
âœ” No side effects

---

#### KPI computation

```python
kpis = SalesKPI(
    total_revenue=round(total_rev, 2),
    total_quantity=round(total_qty, 2),
    total_transactions=int(total_tx),
    average_ticket=round(total_rev / total_tx, 2) if total_tx else 0.0,
    top_products=top_products,
)
```

âœ” Business metrics computed server-side
âœ” Typed output contract

---

### ğŸ“¦ Output Contract

ğŸ“ **File:** `src/models/dto/analytics_dto.py`

```python
class SalesAnalyticsResponse(BaseModel):
    period: AnalyticsPeriod
    start_date: date
    end_date: date
    period_label: str
    kpis: SalesKPI
    daily: List[DailySalesPoint]
```

âœ” Explicit API contract
âœ” Prevents silent schema drift
âœ” Anti-injection by design

---

### ğŸš€ Automation Value

* Eliminates manual sales reporting
* Consistent KPI definitions across the system
* Cached analytics reduce backend load
* Enables downstream pipelines (reports, ML, anomalies)

---

## ğŸ”¹ Stock Analytics Pipeline

### ğŸ¯ Purpose

Provide operational visibility on inventory health:

* Stock valuation
* Low / dead stock detection
* Coverage estimation
* Reorder urgency signals

---

### ğŸ” Pipeline Flow

```
API Request
   â†“
JWT extraction & validation
   â†“
Java Products + Sales APIs
   â†“
Sales history normalization
   â†“
Average daily consumption computation
   â†“
Inventory enrichment
   â†“
Stock KPIs & status classification
   â†“
Redis cache
   â†“
FastAPI response
```

---

### ğŸ§  Implementation â€“ Core Logic

ğŸ“ **File:** `src/services/analytics_service.py`

#### Consumption analysis

```python
qty_by_day = defaultdict(lambda: defaultdict(float))
last_sale: Dict[int, date] = {}

for sale in sales_history:
    d = parse_date(sale.get("saleDate"))
    if not d or d < cutoff:
        continue

    for item in sale.get("items", []) or []:
        pid = int(item.get("productId"))
        qty_by_day[pid][d] += float(item.get("quantity", 0))
        last_sale[pid] = max(d, last_sale.get(pid, d))
```

âœ” Historical consumption tracking
âœ” Fully automated calculation window (90 days)

---

#### Inventory enrichment

```python
enriched = enrich_inventory_with_products_and_sales(
    inventory_rows,
    products,
    last_sale,
    avg_daily,
)
```

âœ” Pipeline composition
âœ” Business logic encapsulated in a dedicated service
âœ” Reusable across analytics and alerting

---

#### KPI computation

```python
kpis = StockKPI(
    total_stock_value=round(sum(s.stock_value for s in snapshots), 2),
    out_of_stock_count=sum(1 for s in snapshots if s.current_stock <= 0),
    low_stock_count=sum(
        1 for s in snapshots if s.status == ProductStockStatus.low.value
    ),
)
```

âœ” Deterministic metrics
âœ” Status-based classification (OK / LOW / DEAD)

---

### ğŸ“¦ Output Contract

ğŸ“ **File:** `src/models/dto/analytics_dto.py`

```python
class StockAnalyticsResponse(BaseModel):
    period: AnalyticsPeriod
    period_label: str
    as_of: date
    kpis: StockKPI
    critical_products: List[ProductStockSnapshot]
```

âœ” Strongly typed response
âœ” Prevents malformed analytics exposure

---

### ğŸš€ Automation Value

* Continuous inventory health computation
* No spreadsheet-based stock analysis
* Enables anomaly detection & alert pipelines
* Predictable, cache-backed performance

---

## ğŸ§  Why This Is a Real Automation Pipeline

âœ” Not a simple function
âœ” Multi-step, cross-service data flow
âœ” Stateless execution
âœ” Typed contracts at each boundary
âœ” Cache-backed delivery
âœ” Reusable by reports, ML, and alerting layers

This analytics layer acts as a **core backend automation engine**, not just a reporting API.


Parfait ğŸ‘
Avec ce que tu viens de fournir, on peut maintenant Ã©crire **la section Excel Import Pipeline** de `pipelines.md` de faÃ§on **100 % fidÃ¨le Ã  ton implÃ©mentation rÃ©elle**, sans extrapolation.

Ci-dessous, je te donne **UNIQUEMENT la partie Excel Import Pipeline**, prÃªte Ã  Ãªtre **intÃ©grÃ©e telle quelle** dans `4-automation-scripts/pipelines.md`, dans la continuitÃ© de la partie Analytics dÃ©jÃ  faite.

---

## ğŸ”¹ Excel Import Pipeline (Sales Data)

### ğŸ¯ Purpose

Automate the ingestion of sales data from **Excel / CSV files** into the SalesFlow system, while enforcing:

* strict input validation,
* canonical data transformation,
* and secure bulk insertion into the Java backend.

This pipeline replaces manual data entry and spreadsheet-based imports with a **controlled, traceable backend workflow**.

---

### ğŸ” Pipeline Flow

```
File Upload (Excel / CSV)
   â†“
Header normalization & row parsing
   â†“
Schema & business rule validation
   â†“
Product resolution (id / sku / name)
   â†“
Canonical transformation
   â†“
Bulk API call to Java backend
   â†“
Structured import result (success / errors)
```

---

### ğŸ§  Implementation â€“ Core Components

---

### 1ï¸âƒ£ Static Import Schema (Security Boundary)

ğŸ“ **File:** `src/models/excel_schemas.py`

```python
REQUIRED_SALES_SCHEMA = {
    "__rules__": {
        "one_of": [["product_id", "sku", "name"]]
    },
    "quantity": {
        "type": "float",
        "required": True,
        "rules": [">0"],
    },
    "sale_date": {
        "type": "date",
        "required": True,
        "format": "%Y-%m-%d",
    }
}
```

âœ” Static schema only (no logic, no I/O)
âœ” Enforces **anti-injection by structure**
âœ” Prevents malformed or ambiguous payloads

This schema defines the **only allowed contract** for uploaded sales files.

---

### 2ï¸âƒ£ File Parsing & Normalization (CSV / XLSX)

ğŸ“ **File:** `src/data/file_processor.py`

```python
def _norm(s: str) -> str:
    return s.strip().lower().replace(" ", "_").replace("-", "_")
```

```python
async def _read_file(file: UploadFile) -> List[Dict[str, Any]]:
    if file.filename.endswith(".csv"):
        reader = csv.DictReader(io.StringIO(text))
        return [{_norm(k): v for k, v in row.items()} for row in reader]
```

âœ” Header normalization prevents format drift
âœ” Supports CSV and Excel transparently
âœ” Ignores empty rows safely

---

### 3ï¸âƒ£ Validation & Canonicalization Pipeline

ğŸ“ **File:** `src/data/file_processor.py`

```python
if not row.get("product_id") and not row.get("sku") and not row.get("name"):
    errors.append({
        "row": row_index,
        "field": "product_id / sku / name",
        "reason": "One of product_id, sku or name is required",
    })
    continue
```

```python
quantity = _to_float(row.get("quantity"))
if quantity is None or quantity <= 0:
    errors.append({
        "row": row_index,
        "field": "quantity",
        "reason": "Invalid or <= 0",
    })
    continue
```

âœ” Per-row validation
âœ” Fail-safe behavior (row-level errors, not global crash)
âœ” Clear feedback for end users

---

### 4ï¸âƒ£ Product Resolution (Cross-Service Automation)

ğŸ“ **File:** `src/data/file_processor.py`

```python
products_client = JavaProductsClient(token)
products = await products_client.get_all_products()
```

```python
if pid and pid in by_id:
    product = by_id[pid]
elif row.get("sku"):
    product = by_sku.get(row["sku"].lower())
elif row.get("name"):
    product = by_name.get(row["name"].lower())
```

âœ” Automated join with Java Product service
âœ” Supports multiple identifiers
âœ” Strong decoupling between Excel format and Java API

---

### 5ï¸âƒ£ Canonical Output (Java-Compatible Payload)

```python
items.append({
    "productId": product.id,
    "sku": product.sku,
    "quantity": int(quantity),
})
```

âœ” Output strictly matches Java `/sales/bulk` contract
âœ” No Excel-specific fields leaked downstream

---

### 6ï¸âƒ£ Secure Bulk Import into Java Backend

ğŸ“ **File:** `src/api/routes/excel.py`

```python
@router.post("/import-sales")
async def import_sales(file: UploadFile, current_user=Depends(get_current_user)):
```

```python
java_response = await sales_client.create_bulk_sales(
    result["items"]
)
```

âœ” JWT extracted once at the API boundary
âœ” Secure service-to-service call
âœ” Atomic bulk operation

---

### ğŸ“¦ Pipeline Output

```json
{
  "status": "success",
  "total_rows": 120,
  "valid_rows": 115,
  "imported": 115
}
```

or, in case of validation issues:

```json
{
  "status": "failed",
  "errors": [
    { "row": 5, "field": "quantity", "reason": "Invalid or <= 0" }
  ]
}
```

âœ” Deterministic results
âœ” Fully traceable import process

---

### ğŸš€ Automation Value

* Eliminates manual sales imports
* Prevents data corruption at ingestion time
* Enforces a strict contract between Excel and backend services
* Reduces operational errors during bulk imports
* Integrates seamlessly with Analytics & ML pipelines

---

### ğŸ§  Why This Is a Real Automation Pipeline

âœ” Multi-step backend workflow
âœ” Cross-service orchestration (Python â†” Java)
âœ” Static schema + runtime validation
âœ” Canonical transformation layer
âœ” Secure, repeatable, and auditable execution

This Excel Import Pipeline is not a helper script â€”
it is a **production-grade backend automation pipeline**.

---


Parfait. LÃ  on a **tout ce quâ€™il faut** pour Ã©crire une section **ML Pipeline** qui est :

* 100 % fidÃ¨le Ã  ton code
* crÃ©dible niveau **Data / Backend / DevSecOps**
* prÃªte Ã  Ãªtre **commitÃ©e telle quelle** dans `4-automation-scripts/pipelines.md`

Je te donne **la version finale de la section â€œMachine Learning Pipelineâ€**, en **anglais**, **Markdown**, avec **extraits exacts** (lÃ©gÃ¨rement raccourcis mais inchangÃ©s sÃ©mantiquement).

---

# ğŸ¤– Machine Learning Pipeline (Forecasting & Anomaly Detection)

The Machine Learning layer in **SalesFlow Lite** is implemented as a **fully automated backend pipeline**, not as isolated ML scripts.
It integrates data collection, preprocessing, model training, inference, anomaly detection, alerting, caching, and API exposure.

This pipeline supports **GLOBAL** and **PRODUCT-level** analytics, with automatic SKU or product name resolution.

---

## ğŸ“Œ Pipeline Scope

* **Forecasting**: Short-term sales prediction (7 / 30 / 90 days)
* **Anomaly Detection**: Detection of abnormal sales patterns
* **Automation Level**: ML-assisted decision support (human-in-the-loop)
* **Execution Mode**: On-demand via API + cached results

---

## ğŸ”„ End-to-End Pipeline Flow

```
Java Sales API
   â†“
Sales history loading (SKU or NAME resolution)
   â†“
Data cleaning & preprocessing
   â†“
Regression model training (Linear Regression)
   â†“
Forecast inference
   â†“
Anomaly detection (Z-score)
   â†“
Severity classification
   â†“
Alert dispatch (log / flag / email)
   â†“
Redis cache
   â†“
FastAPI response
```

---

## ğŸ§  Step 1 â€” Automated Data Collection (Java â†’ Python)

ğŸ“ **File:** `src/services/ml_service.py`

The pipeline automatically loads historical sales data from the Java backend.
It supports:

* GLOBAL sales
* PRODUCT sales by **SKU**
* PRODUCT sales by **name** (resolved via Products API)

```python
async def _load_sales_history(
    scope: ForecastScope,
    product_sku: Optional[str],
    product_name: Optional[str],
    token: Optional[str],
) -> pd.DataFrame:
```

```python
client = JavaSalesClient(token)

if scope == ForecastScope.GLOBAL:
    sales = await client.get_sales_history()
else:
    if product_sku:
        flat = await client.get_sales_history_by_sku(product_sku)
    elif product_name:
        prod_client = JavaProductsClient(token)
        product = await prod_client.get_product_by_name(product_name)
        flat = await client.get_sales_history_by_sku(product.sku)
```

âœ… **Automation value**

* No manual data extraction
* Dynamic product resolution
* Secure service-to-service communication (JWT)

---

## ğŸ§¼ Step 2 â€” Data Cleaning & Preprocessing Pipeline

ğŸ“ **Files:**

* `src/data/ml_preprocessor.py`
* `src/services/ml_service.py`

```python
df = clean_numeric(df, "quantity")
df = fill_missing_values(df)
```

```python
X, y, dfp = prepare_regression_features(df)
```

âœ” Ensures:

* Numeric consistency
* Missing value handling
* Deterministic feature generation

---

## ğŸ“ˆ Step 3 â€” Automated Model Training & Forecasting

ğŸ“ **File:** `src/services/ml_service.py`

A **Linear Regression** model is trained dynamically using recent historical data.

```python
model = LinearRegression().fit(X, y)

future_idx = np.arange(
    last_idx + 1,
    last_idx + 1 + forecast_days
).reshape(-1, 1)

preds = model.predict(future_idx).clip(0).round(2).tolist()
```

The pipeline automatically produces:

* Future dates
* Predictions
* Trend classification (upward / downward / stable)

```python
trend = (
    "upward" if preds[-1] > preds[0]
    else "downward" if preds[-1] < preds[0]
    else "stable"
)
```

---

## ğŸš¨ Step 4 â€” Anomaly Detection Pipeline

ğŸ“ **File:** `src/services/ml_service.py`

Anomaly detection is implemented using **Z-score analysis** on historical quantities.

```python
values = df["quantity"].astype(float)
mean, std = values.mean(), values.std()

z = (values - mean) / std
```

```python
if abs(score) >= 3:
    anomalies.append({
        "date": ...,
        "value": ...,
        "score": score,
        "severity": "high" if abs(score) >= 4 else "medium",
        "type": "HIGH_SPIKE" if score > 0 else "DROP",
    })
```

âœ” Deterministic
âœ” Explainable
âœ” Safe for business usage

---

## ğŸ”” Step 5 â€” Alert Automation (Human-in-the-loop)

ğŸ“ **File:** `src/services/anomaly_alert_service.py`

Each detected anomaly is passed to a centralized alert handler.

```python
handle_anomaly_alert(
    anomaly=anomaly,
    scope=scope.value,
    product_sku=product_sku,
    product_name=product_name,
    period=period.value,
)
```

Severity-based behavior:

* **LOW** â†’ logged
* **MEDIUM** â†’ flagged in analytics output
* **HIGH** â†’ email alert triggered

ğŸš« No auto-remediation
âœ… Decision-support automation only

---

## âš¡ Step 6 â€” Caching & Performance Automation

ğŸ“ **File:** `src/data/cache_manager.py`

Forecast and anomaly results are cached automatically.

```python
cache_key = f"ml:forecast:{scope}:{identifier}"
cached = get_cache(cache_key)

if cached:
    return json.loads(cached)
```

```python
set_cache(cache_key, json.dumps(result), TTL_ANALYTICS)
```

âœ” Reduces recomputation
âœ” Improves API latency
âœ” Protects Java backend from repeated calls

---

## ğŸ¨ Step 7 â€” UI-Oriented Result Enrichment

ğŸ“ **File:** `src/services/ml_enrichment_service.py`

ML results are enriched with product metadata **without touching ML logic**.

```python
def enrich_ml_result(result, product):
    base = dict(result)
    if product:
        base["product"] = {
            "id": product.id,
            "name": product.name,
            "sku": product.sku,
            "price": product.price,
        }
```

âœ” Clean separation of concerns
âœ” ML stays ML
âœ” UI gets context

---

## ğŸŒ Step 8 â€” API Exposure (Triggering the Pipeline)

ğŸ“ **File:** `src/api/routes/ml.py`

The pipeline is triggered via secure FastAPI endpoints.

```python
@router.post("/forecast")
async def forecast_endpoint(payload, current_user):
    ml_result = await forecast_sales(...)
    return enrich_ml_result(ml_result, product)
```

```python
@router.post("/anomalies")
async def anomalies_endpoint(payload, current_user):
    result = await detect_anomalies(...)
    return enrich_ml_result(result, product)
```

---

## ğŸ¯ Why This Is a Real ML Pipeline

âœ” Not a notebook
âœ” Not a standalone script
âœ” Not a demo ML model

This pipeline demonstrates:

* End-to-end automation
* Secure inter-service data flow
* Deterministic ML behavior
* Explainable anomaly detection
* Production-oriented design

---




Parfait.
Avec ce que tu viens dâ€™envoyer, on peut **complÃ©ter et verrouiller dÃ©finitivement la partie â€œAnomaly Detection & Alert Pipelineâ€** dans `pipelines.md`, **avec des extraits EXACTS**, sans enjoliver.

Je te donne **la version finale Ã  insÃ©rer telle quelle** (anglais, Markdown, portfolio-ready).

---

# ğŸš¨ Anomaly Detection & Alert Pipeline (ML-Assisted Automation)

This pipeline extends the Machine Learning analytics layer by adding **operational alerting**, transforming anomaly detection into an actionable, production-oriented workflow.

It is intentionally designed as **human-in-the-loop automation**: the system detects, classifies, and notifies, but does not perform automatic remediation.

---

## ğŸ”„ Pipeline Flow

```
Sales history
   â†“
ML-based anomaly detection (Z-score)
   â†“
Severity classification
   â†“
Alert policy enforcement
   â†“
Email notification (SMTP)
   â†“
Operational awareness
```

---

## ğŸ§  Step 1 â€” Anomaly Detection (Upstream ML Pipeline)

ğŸ“ **File:** `src/services/ml_service.py`

Anomalies are detected using statistical Z-score analysis on historical sales quantities.
Each anomaly includes a severity level (`medium` or `high`) and an explanation.

```python
if abs(score) >= 3:
    anomalies.append({
        "date": df["date"].iloc[i].strftime("%Y-%m-%d"),
        "value": float(values.iloc[i]),
        "score": round(float(score), 3),
        "severity": "high" if abs(score) >= 4 else "medium",
        "type": "HIGH_SPIKE" if score > 0 else "DROP",
        "explanation": "Z-score anomaly detected",
    })
```

âœ” Deterministic
âœ” Explainable
âœ” Suitable for business environments

---

## ğŸ”” Step 2 â€” Alert Policy Enforcement (Automation Gate)

ğŸ“ **File:** `src/services/anomaly_alert_service.py`

This component acts as a **policy gate** between ML detection and operational alerting.

```python
def handle_anomaly_alert(
    anomaly: dict,
    scope: str,
    product_sku: Optional[str],
    product_name: Optional[str],
    period: str,
):
    if DEV_MODE:
        return
```

### ğŸ”’ Security & Safety Controls

```python
severity = anomaly.get("severity")

# Alert policy
if severity not in ("high", "medium"):
    return
```

âœ” No alerts in development mode
âœ” Explicit severity filtering
âœ” Prevents alert fatigue

---

## ğŸ“¦ Step 3 â€” Alert Payload Normalization

Before sending alerts, the pipeline builds a **clean, normalized payload**, combining ML output and business context.

```python
payload = {
    "scope": scope,
    "sku": product_sku,
    "name": product_name,
    "period": period,
    **anomaly,
}
```

âœ” Structured
âœ” Traceable
âœ” Ready for external integrations (email, Slack, SIEM, etc.)

---

## ğŸ“§ Step 4 â€” Email Notification Automation (SMTP)

ğŸ“ **File:** `src/services/email_service.py`

High- and medium-severity anomalies trigger **automatic email alerts** using SMTP.

```python
def send_anomaly_email(anomaly: dict):
    subject = f"ğŸš¨ [{anomaly['severity'].upper()}] SalesFlow anomaly"
```

```python
with smtplib.SMTP(SMTP_HOST, SMTP_PORT) as server:
    server.starttls()
    server.login(SMTP_USER, SMTP_PASSWORD)
    server.send_message(msg)
```

### Email Content Includes:

* Scope (GLOBAL / PRODUCT)
* SKU and product name
* Period (daily / weekly)
* Anomaly type
* Date, value, Z-score
* Human-readable explanation

âœ” Fully automated
âœ” Environment-driven configuration
âœ” Production-ready SMTP integration

---

## ğŸ§ª Step 5 â€” Safe Manual Testing Capability

ğŸ“ **File:** `src/services/email_service.py`

A controlled test entry point exists for validating alert delivery.

```python
if __name__ == "__main__":
    send_anomaly_email({
        "severity": "high",
        "scope": "TEST",
        "sku": "TEST-SKU",
        "name": "Test Product",
        "period": "daily",
        "type": "MANUAL_TEST",
        "date": "2025-01-01",
        "value": 999,
        "score": 5.2,
        "explanation": "This is a manual test email"
    })
```

âœ” Safe validation
âœ” No production data involved
âœ” Easy Ops testing

---

## ğŸ¯ Automation Characteristics

| Aspect       | Implementation           |
| ------------ | ------------------------ |
| Trigger      | ML anomaly detection     |
| Policy       | Severity-based filtering |
| Execution    | Fully automated          |
| Safety       | DEV_MODE bypass          |
| Remediation  | âŒ None (intentional)     |
| Notification | Email (SMTP)             |

---

## ğŸ§  Why This Pipeline Matters

This alerting pipeline demonstrates:

* **ML-driven operational automation**
* Clear separation between detection and notification
* Environment-aware execution (Dev vs Prod)
* Explainable alerts suitable for business users
* A DevSecOps mindset: *detect â†’ classify â†’ notify*

It avoids the common pitfall of unsafe auto-remediation while still providing **actionable intelligence**.

---

## ğŸ Pipeline Classification

**Automation Level:**
âœ… Intelligent assistance (decision support)
âš ï¸ No autonomous remediation (by design)

---

### âœ… RÃ©sultat

Avec cette section + la ML pipeline prÃ©cÃ©dente, ton `pipelines.md` montre clairement que tu sais :

* Concevoir des **pipelines ML exploitables**
* Aller jusquâ€™Ã  lâ€™**alerte opÃ©rationnelle**
* Appliquer des **politiques de sÃ©curitÃ© et de sÃ»retÃ©**
* Travailler comme un **Cloud / Data / Security Engineer**

---
